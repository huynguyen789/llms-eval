{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.conda/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: openpyxl in ./.conda/lib/python3.8/site-packages (3.1.5)\n",
      "Requirement already satisfied: openai in ./.conda/lib/python3.8/site-packages (1.40.0)\n",
      "Collecting anthropic\n",
      "  Downloading anthropic-0.32.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.8/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.conda/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.conda/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: et-xmlfile in ./.conda/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.conda/lib/python3.8/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.conda/lib/python3.8/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.8/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.conda/lib/python3.8/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.conda/lib/python3.8/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in ./.conda/lib/python3.8/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.conda/lib/python3.8/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.conda/lib/python3.8/site-packages (from openai) (4.12.2)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic)\n",
      "  Downloading tokenizers-0.19.1-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.conda/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.conda/lib/python3.8/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.0->anthropic)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.2.2)\n",
      "Downloading anthropic-0.32.0-py3-none-any.whl (866 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.6/866.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp38-cp38-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: fsspec, filelock, huggingface-hub, tokenizers, anthropic\n",
      "Successfully installed anthropic-0.32.0 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.24.5 tokenizers-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pandas openpyxl openai anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "  - Test gpt 4o mini fine-tuning.\n",
    "\n",
    "# Plan:\n",
    "- Create an eval pipeline\n",
    "- Fine-tuning gpt 4o mini\n",
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval multi openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the original JSON file\n",
    "input_file = '/Users/huyknguyen/Desktop/redhorse/code_projects/eval/eval/EvalDataset-20.json'\n",
    "output_file = '/Users/huyknguyen/Desktop/redhorse/code_projects/eval/eval/EvalDataset-20-openaiFormat.jsonl'\n",
    "\n",
    "# Read the input file\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Transform the data into openai format and write to JSONL file\n",
    "with open(output_file, 'w') as f:\n",
    "    for item in data:\n",
    "        transformed_item = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are the smartest entity in the universe. Reasoning step by step to get the best answer.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": item[\"instruction\"]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": item[\"output\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        # Write each transformed item as a single line in the output file\n",
    "        f.write(json.dumps(transformed_item) + '\\n')\n",
    "\n",
    "print(f\"Transformation complete. Output written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class mathematician. First briefly repeat the answer in 1 sentence, then reasoning step by step to make sure you get the correct answer.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=model_name,\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. Your task is to compare the model's answer with the expected output and provide a score from 0 to 10. Use the following scale: 0 is completely wrong, 10 is completely correct, 8-9 if correct but missing detail or not a complete answer. Dont grade on formatting, as long as the answer is correct. Always include the numeric score (0-10) in your response.\"},\n",
    "      {\"role\": \"user\", \"content\": f\"Model answer: {model_answer}\\n\\nExpected output: {expected_output}\\n\\nPlease evaluate and provide a score from 0 to 10, no text or explanations needed.\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score(evaluation):\n",
    "  match = re.search(r'\\b(?:10|[0-9])\\b', evaluation)\n",
    "  if match:\n",
    "      return int(match.group())\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score from evaluation: {evaluation}\")\n",
    "      return 0  # Default to 0 if we can't extract a valid score\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score = extract_score(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, evaluation, score\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "  tasks = [process_item(item, model_name) for item in eval_data]\n",
    "  results = await asyncio.gather(*tasks)\n",
    "\n",
    "  df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Evaluation', 'Score'])\n",
    "  avg_score = df['Score'].mean()\n",
    "  \n",
    "  print(f\"\\nModel: {model_name}\")\n",
    "  print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "  # Create output directory if it doesn't exist\n",
    "  os.makedirs('./output', exist_ok=True)\n",
    "\n",
    "  # Remove the 'Evaluation' column before saving to Excel\n",
    "  df_to_save = df.drop(columns=['Evaluation'])\n",
    "\n",
    "  excel_path = f'./output/evaluation_results_{model_name}.xlsx'\n",
    "  df_to_save.to_excel(excel_path, index=False)\n",
    "  print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "  return df, avg_score\n",
    "async def main():\n",
    "  models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o\"]  # Add your model names here\n",
    "  results = {}\n",
    "\n",
    "  for model in models_to_evaluate:\n",
    "      df, avg_score = await evaluate_model(model)\n",
    "      results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "  # Create a summary DataFrame\n",
    "  summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "  summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "  summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  print(\"\\nModel Comparison Summary:\")\n",
    "  display(summary_df)\n",
    "\n",
    "  # Save summary to Excel in the output folder\n",
    "  summary_excel_path = './output/model_comparison_summary.xlsx'\n",
    "  summary_df.to_excel(summary_excel_path, index=False)\n",
    "  print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "  return results, summary_df\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "  get_ipython()\n",
    "  is_notebook = True\n",
    "except NameError:\n",
    "  is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "  # If in a Jupyter notebook, use this:\n",
    "  results, summary_df = await main()\n",
    "else:\n",
    "  # If in a regular Python script, use this:\n",
    "  results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n",
      "Average Evaluation Score: 7.69\n",
      "Results saved to ./output/evaluation_results_gpt-4o-mini.xlsx\n",
      "\n",
      "Model: gpt-4o\n",
      "Average Evaluation Score: 8.94\n",
      "Results saved to ./output/evaluation_results_gpt-4o.xlsx\n",
      "\n",
      "Model: gpt-4-0125-preview\n",
      "Average Evaluation Score: 9.31\n",
      "Results saved to ./output/evaluation_results_gpt-4-0125-preview.xlsx\n",
      "\n",
      "Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>9.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>8.9375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>7.6875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Average Score\n",
       "0  gpt-4-0125-preview         9.3125\n",
       "1              gpt-4o         8.9375\n",
       "2         gpt-4o-mini         7.6875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary saved to ./output/model_comparison_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step to make sure you get the correct answer.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=model_name,\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. \"},\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"\n",
    "       Model answer: {model_answer}\\n\\n\n",
    "       Expected output: {expected_output}\\n\\n\n",
    "       Your task is to compare the model's answer WITH THE EXPECTED OUTPUT and provide a super concise reason in one short sentence for the score, and then a score from 0 to 10. \n",
    "       Example: Reason: [super concise reason here]. Score: [score here]. \n",
    "       Use the following scale: 0 is completely wrong, 10 is completely correct, 8-9 if correct but missing detail or not a complete answer. \n",
    "       Don't grade on formatting, as long as the answer is correct compare to the expected output. \n",
    "       If the logic is correct but the final answer is wrong, it's still wrong.\n",
    "       If the answer is correct but it has extra information, it's still correct. As long as the extra info is not completely wrong or hallucinated.\n",
    "       Do not grade by your knowledge, but grade based on the expected output. \n",
    "       Always include the numeric score (0-10) in your response.\n",
    "       \"\"\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score_and_reason(evaluation):\n",
    "  match = re.search(r'Reason:\\s*(.*?)\\s*Score:\\s*(\\d+|10)', evaluation, re.IGNORECASE | re.DOTALL)\n",
    "  if match:\n",
    "      reason = match.group(1).strip()\n",
    "      score = int(match.group(2))\n",
    "      return score, reason\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score and reason from evaluation: {evaluation}\")\n",
    "      return 0, \"Unable to extract reason\"  # Default values if extraction fails\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score, reason = extract_score_and_reason(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, score, reason\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "  tasks = [process_item(item, model_name) for item in eval_data]\n",
    "  results = await asyncio.gather(*tasks)\n",
    "\n",
    "  df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Score', 'Reason'])\n",
    "  avg_score = df['Score'].mean()\n",
    "  \n",
    "  print(f\"\\nModel: {model_name}\")\n",
    "  print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "  # Create output directory if it doesn't exist\n",
    "  os.makedirs('./output', exist_ok=True)\n",
    "\n",
    "  excel_path = f'./output/evaluation_results_{model_name}.xlsx'\n",
    "  df.to_excel(excel_path, index=False)\n",
    "  print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "  return df, avg_score\n",
    "\n",
    "async def main():\n",
    "  models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-0125-preview\"]  # Add your model names here\n",
    "  results = {}\n",
    "\n",
    "  for model in models_to_evaluate:\n",
    "      df, avg_score = await evaluate_model(model)\n",
    "      results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "  # Create a summary DataFrame\n",
    "  summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "  summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "  summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  print(\"\\nModel Comparison Summary:\")\n",
    "  display(summary_df)\n",
    "\n",
    "  # Save summary to Excel in the output folder\n",
    "  summary_excel_path = './output/model_comparison_summary.xlsx'\n",
    "  summary_df.to_excel(summary_excel_path, index=False)\n",
    "  print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "  return results, summary_df\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "  get_ipython()\n",
    "  is_notebook = True\n",
    "except NameError:\n",
    "  is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "  # If in a Jupyter notebook, use this:\n",
    "  results, summary_df = await main()\n",
    "else:\n",
    "  # If in a regular Python script, use this:\n",
    "  results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval different models:\n",
    "- Eval pipeline for other models: google, anthropic, open sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
