{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas openpyxl openai anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval multi openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n",
      "Average Evaluation Score: 85.00\n",
      "Results saved to ./output/EvalDataset-20_081224_1229PM/EvalDataset-20_081224_1229PM_gpt-4o-mini.xlsx\n",
      "\n",
      "Model: gpt-4o-2024-08-06\n",
      "Average Evaluation Score: 92.00\n",
      "Results saved to ./output/EvalDataset-20_081224_1229PM/EvalDataset-20_081224_1229PM_gpt-4o-2024-08-06.xlsx\n",
      "\n",
      "Model: gpt-4o-2024-05-13\n",
      "Average Evaluation Score: 91.00\n",
      "Results saved to ./output/EvalDataset-20_081224_1229PM/EvalDataset-20_081224_1229PM_gpt-4o-2024-05-13.xlsx\n",
      "\n",
      "Model: gpt-4-0125-preview\n",
      "Average Evaluation Score: 86.50\n",
      "Results saved to ./output/EvalDataset-20_081224_1229PM/EvalDataset-20_081224_1229PM_gpt-4-0125-preview.xlsx\n",
      "\n",
      "Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>86.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Average Score\n",
       "0   gpt-4o-2024-08-06           92.0\n",
       "1   gpt-4o-2024-05-13           91.0\n",
       "2  gpt-4-0125-preview           86.5\n",
       "3         gpt-4o-mini           85.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary saved to ./output/EvalDataset-20_081224_1229PM/model_comparison_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "from anthropic import AsyncAnthropic\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "anthropic_client = AsyncAnthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step to make sure you get the correct answer.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=model_name,\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. \"},\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"\n",
    "       Model answer: {model_answer}\\n\\n\n",
    "       Expected output: {expected_output}\\n\\n\n",
    "       Your task is to compare the model's answer WITH THE EXPECTED OUTPUT and provide a super concise reason in one short sentence for the score, and then a score from 0 to 100. \n",
    "       Example: Reason: [super concise reason here]. Score: [score here]. \n",
    "       Use the following scale: 0 is completely wrong, 50 is missing half of the solution, 100 is completely correct, 80-90 if correct but missing some detail or not a complete answer. \n",
    "       Don't grade on formatting, as long as the answer is correct compare to the expected output. \n",
    "       If the logic is correct but the final answer is wrong, it's still wrong.\n",
    "       If the answer is correct but it has extra information, it's still correct. As long as the extra info is not completely wrong or hallucinated.\n",
    "       Do not grade by your knowledge, but grade based on the expected output. \n",
    "       Always include the numeric score (0-10) in your response.\n",
    "       \"\"\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score_and_reason(evaluation):\n",
    "  match = re.search(r'Reason:\\s*(.*?)\\s*Score:\\s*(\\d+|10)', evaluation, re.IGNORECASE | re.DOTALL)\n",
    "  if match:\n",
    "      reason = match.group(1).strip()\n",
    "      score = int(match.group(2))\n",
    "      return score, reason\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score and reason from evaluation: {evaluation}\")\n",
    "      return 0, \"Unable to extract reason\"  # Default values if extraction fails\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score, reason = extract_score_and_reason(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, score, reason\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "    tasks = [process_item(item, model_name) for item in eval_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Score', 'Reason'])\n",
    "    avg_score = df['Score'].mean()\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "    excel_path = f'{output_folder}/{dataset_name}_{current_time}_{model_name}.xlsx'\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "    return df, avg_score\n",
    "\n",
    "async def main():\n",
    "    models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"gpt-4o-2024-05-13\", \"gpt-4-0125-preview\"]  # Add your model names here\n",
    "    results = {}\n",
    "\n",
    "    for model in models_to_evaluate:\n",
    "        df, avg_score = await evaluate_model(model)\n",
    "        results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "    # Create a summary DataFrame\n",
    "    summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "    summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "    summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    # Save summary to Excel in the output folder\n",
    "    summary_excel_path = f'{output_folder}/model_comparison_summary.xlsx'\n",
    "    summary_df.to_excel(summary_excel_path, index=False)\n",
    "    print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "    return results, summary_df\n",
    "\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/EvalDataset-20.json'\n",
    "# './input/huy_dataset/huy_test.json'\n",
    "# 'input/math-EvalDataset-10.json'\n",
    "#'./input/huy_dataset/huy_test2.json'\n",
    "# './input/EvalDataset-20.json'\n",
    "\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "# Extract dataset name from input file path\n",
    "dataset_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "# Create output folder name\n",
    "current_time = datetime.now().strftime(\"%m%d%y_%I%M%p\")\n",
    "output_folder = f'./output/{dataset_name}_{current_time}'\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "    get_ipython()\n",
    "    is_notebook = True\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "    # If in a Jupyter notebook, use this:\n",
    "    results, summary_df = await main()\n",
    "else:\n",
    "    # If in a regular Python script, use this:\n",
    "    results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval structure output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n",
      "Average Evaluation Score: 78.12\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4o-mini.xlsx\n",
      "\n",
      "Model: gpt-4o\n",
      "Average Evaluation Score: 98.44\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4o.xlsx\n",
      "\n",
      "Model: gpt-4o-2024-08-06\n",
      "Average Evaluation Score: 94.06\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4o-2024-08-06.xlsx\n",
      "\n",
      "Model: gpt-4-0125-preview\n",
      "Average Evaluation Score: 86.25\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4-0125-preview.xlsx\n",
      "\n",
      "Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>98.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>94.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>86.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>78.1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Average Score\n",
       "0              gpt-4o        98.4375\n",
       "1   gpt-4o-2024-08-06        94.0625\n",
       "2  gpt-4-0125-preview        86.2500\n",
       "3         gpt-4o-mini        78.1250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary saved to ./output/huy_test_081024_1208PM/model_comparison_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Step(BaseModel):\n",
    "  explanation: str\n",
    "  output: str\n",
    "\n",
    "class MathResponse(BaseModel):\n",
    "  steps: List[Step]\n",
    "  final_answer: str\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step and consider multi angles to make sure you get the correct and complete answer.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  \n",
    "  if model_name == \"gpt-4o-2024-08-06\":\n",
    "      response = await client.beta.chat.completions.parse(\n",
    "          model=model_name,\n",
    "          messages=messages,\n",
    "          response_format=MathResponse,\n",
    "      )\n",
    "      message = response.choices[0].message\n",
    "      if message.parsed:\n",
    "          steps_text = \"\\n\".join([f\"Step {i+1}: {step.explanation} Output: {step.output}\" for i, step in enumerate(message.parsed.steps)])\n",
    "          final_answer_text = f\"Final Answer: {message.parsed.final_answer}\"\n",
    "          combined_answer = f\"{steps_text}\\n{final_answer_text}\"\n",
    "          return combined_answer\n",
    "      else:\n",
    "          return message.refusal\n",
    "  else:\n",
    "      response = await client.chat.completions.create(\n",
    "          model=model_name,\n",
    "          messages=messages,\n",
    "          temperature=0.0,\n",
    "      )\n",
    "      return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. \"},\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"\n",
    "      Model answer: {model_answer}\\n\\n\n",
    "      Expected output: {expected_output}\\n\\n\n",
    "      Your task is to compare the model's answer WITH THE EXPECTED OUTPUT and provide a super concise reason in one short sentence for the score, and then a score from 0 to 100. \n",
    "      Example: Reason: [super concise reason here]. Score: [score here]. \n",
    "      Use the following scale: 0 is completely wrong, 100 is completely correct, 80-90 if correct but missing detail or not a complete answer. \n",
    "      Don't grade on formatting, as long as the answer is correct compare to the expected output. \n",
    "      If the logic is correct but the final answer is wrong, it's still wrong.\n",
    "      If the answer is correct but it has extra information, it's still correct. As long as the extra info is not completely wrong or hallucinated.\n",
    "      Do not grade by your knowledge, but grade based on the expected output. \n",
    "      Always include the numeric score (0-100) in your response.\n",
    "      \"\"\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4-0125-preview\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score_and_reason(evaluation):\n",
    "  match = re.search(r'Reason:\\s*(.*?)\\s*Score:\\s*(\\d+|100)', evaluation, re.IGNORECASE | re.DOTALL)\n",
    "  if match:\n",
    "      reason = match.group(1).strip()\n",
    "      score = int(match.group(2))\n",
    "      return score, reason\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score and reason from evaluation: {evaluation}\")\n",
    "      return 0, \"Unable to extract reason\"  # Default values if extraction fails\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score, reason = extract_score_and_reason(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, score, reason\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "  tasks = [process_item(item, model_name) for item in eval_data]\n",
    "  results = await asyncio.gather(*tasks)\n",
    "\n",
    "  df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Score', 'Reason'])\n",
    "  avg_score = df['Score'].mean()\n",
    "  \n",
    "  print(f\"\\nModel: {model_name}\")\n",
    "  print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "  excel_path = f'{output_folder}/{dataset_name}_{current_time}_{model_name}.xlsx'\n",
    "  df.to_excel(excel_path, index=False)\n",
    "  print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "  return df, avg_score\n",
    "\n",
    "async def main():\n",
    "  models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4o-2024-08-06\", \"gpt-4-0125-preview\"]  # Add your model names here\n",
    "  results = {}\n",
    "\n",
    "  for model in models_to_evaluate:\n",
    "      df, avg_score = await evaluate_model(model)\n",
    "      results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "  # Create a summary DataFrame\n",
    "  summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "  summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "  summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  print(\"\\nModel Comparison Summary:\")\n",
    "  display(summary_df)\n",
    "\n",
    "  # Save summary to Excel in the output folder\n",
    "  summary_excel_path = f'{output_folder}/model_comparison_summary.xlsx'\n",
    "  summary_df.to_excel(summary_excel_path, index=False)\n",
    "  print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "  return results, summary_df\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "# Extract dataset name from input file path\n",
    "dataset_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "# Create output folder name\n",
    "current_time = datetime.now().strftime(\"%m%d%y_%I%M%p\")\n",
    "output_folder = f'./output/{dataset_name}_{current_time}'\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "  get_ipython()\n",
    "  is_notebook = True\n",
    "except NameError:\n",
    "  is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "  # If in a Jupyter notebook, use this:\n",
    "  results, summary_df = await main()\n",
    "else:\n",
    "  # If in a regular Python script, use this:\n",
    "  results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval different models:\n",
    "- Eval pipeline for other models: google, anthropic, open sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "from anthropic import AsyncAnthropic\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "anthropic_client = AsyncAnthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "async def get_openai_answer(instruction, model_name):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step and consider multiple angles to make sure you get the correct answer(s).\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        tool_choice=None\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def get_anthropic_answer(instruction, model_name):\n",
    "  message = await anthropic_client.messages.create(\n",
    "      model=model_name,\n",
    "      max_tokens=1024,\n",
    "      temperature=0,\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"You are the most intelligent entity in the universe. Reasoning step by step and consider multiple angles to make sure you get the correct answer(s). Here's the task: {instruction}\",\n",
    "          }\n",
    "      ],\n",
    "  )\n",
    "  return message.content[0].text\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. \"},\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"\n",
    "       Model answer: {model_answer}\\n\\n\n",
    "       Expected output: {expected_output}\\n\\n\n",
    "       Your task is to compare the model's answer WITH THE EXPECTED OUTPUT and provide a super concise reason in one short sentence for the score, and then a score from 0 to 100. \n",
    "       Example: Reason: [super concise reason here]. Score: [score here]. \n",
    "       Use the following scale: 0 is completely wrong, 50 is missing half of the solution, 100 is completely correct, 80-90 if correct but missing some detail or not a complete answer. \n",
    "       Don't grade on formatting, as long as the answer is correct compare to the expected output. \n",
    "       If the logic is correct but the final answer is wrong, it's still wrong.\n",
    "       If the answer is correct but it has extra information, it's still correct. As long as the extra info is not completely wrong or hallucinated.\n",
    "       Do not grade by your knowledge, but grade based on the expected output. \n",
    "       Always include the numeric score (0-100) in your response.\n",
    "       \"\"\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score_and_reason(evaluation):\n",
    "  match = re.search(r'Reason:\\s*(.*?)\\s*Score:\\s*(\\d+|10)', evaluation, re.IGNORECASE | re.DOTALL)\n",
    "  if match:\n",
    "      reason = match.group(1).strip()\n",
    "      score = int(match.group(2))\n",
    "      return score, reason\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score and reason from evaluation: {evaluation}\")\n",
    "      return 0, \"Unable to extract reason\"  # Default values if extraction fails\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  if model_name.startswith(\"claude\"):\n",
    "      model_answer = await get_anthropic_answer(item['instruction'], model_name)\n",
    "  else:\n",
    "      model_answer = await get_openai_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score, reason = extract_score_and_reason(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, score, reason\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "    tasks = [process_item(item, model_name) for item in eval_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Score', 'Reason'])\n",
    "    avg_score = df['Score'].mean()\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "    excel_path = f'{output_folder}/{dataset_name}_{current_time}_{model_name}.xlsx'\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "    return df, avg_score\n",
    "\n",
    "async def main():\n",
    "  models_to_evaluate = [\n",
    "      \"claude-3-5-sonnet-20240620\",\n",
    "      \"gpt-4o-mini\", \n",
    "      \"gpt-4o-2024-08-06\", \n",
    "      \"gpt-4o-2024-05-13\", \n",
    "      \"gpt-4-0125-preview\"\n",
    "  ]\n",
    "  results = {}\n",
    "\n",
    "  for model in models_to_evaluate:\n",
    "      df, avg_score = await evaluate_model(model)\n",
    "      results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "  # Create a summary DataFrame\n",
    "  summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "  summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "  summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  print(\"\\nModel Comparison Summary:\")\n",
    "  print(summary_df)\n",
    "\n",
    "  # Save summary to Excel in the output folder\n",
    "  summary_excel_path = f'{output_folder}/model_comparison_summary.xlsx'\n",
    "  summary_df.to_excel(summary_excel_path, index=False)\n",
    "  print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "  return results, summary_df\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "# './input/huy_dataset/huy_test.json'\n",
    "# 'input/math-EvalDataset-10.json'\n",
    "#'./input/huy_dataset/huy_test2.json'\n",
    "# './input/EvalDataset-20.json'\n",
    "\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "# Extract dataset name from input file path\n",
    "dataset_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "# Create output folder name\n",
    "current_time = datetime.now().strftime(\"%m%d%y_%I%M%p\")\n",
    "output_folder = f'./output/{dataset_name}_{current_time}'\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "    get_ipython()\n",
    "    is_notebook = True\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "    results, summary_df = await main()\n",
    "else:\n",
    "    results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "import re\n",
    "import nest_asyncio\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "# Apply nest_asyncio to allow running async code in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "# \"chatgpt-4o-latest\"\n",
    "\n",
    "async def get_model_answer(instruction, prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    response = await client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "    eval_prompt = load_prompt('eval_prompt.txt')\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI model evaluator.\"},\n",
    "        {\"role\": \"user\", \"content\": eval_prompt.format(\n",
    "            model_answer=model_answer,\n",
    "            expected_output=expected_output\n",
    "        )}\n",
    "    ]\n",
    "    response = await client.chat.completions.create(\n",
    "        model='gpt-4-0125-preview',\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def extract_score(evaluation):\n",
    "    match = re.search(r'Score:\\s*(\\d+)', evaluation)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "# Function to save a prompt to a file\n",
    "def save_prompt(prompt, filename):\n",
    "    os.makedirs('prompts', exist_ok=True)\n",
    "    with open(os.path.join('prompts', filename), 'w') as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "# Function to load a prompt from a file\n",
    "def load_prompt(filename):\n",
    "    with open(os.path.join('prompts', filename), 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "# New function to remove content inside <analysis> tags\n",
    "def remove_analysis(prompt):\n",
    "    return re.sub(r'<analysis>.*?</analysis>', '', prompt, flags=re.DOTALL)\n",
    "\n",
    "async def optimize_prompt(initial_prompt, dataset, max_iterations=5):\n",
    "    current_prompt = initial_prompt\n",
    "    best_prompt = initial_prompt\n",
    "    best_score = 0\n",
    "    initial_score = None\n",
    "    no_improvement_count = 0\n",
    "    last_scores = deque(maxlen=5)\n",
    "\n",
    "    # Save the initial prompt\n",
    "    save_prompt(initial_prompt, f'prompt_iteration_0.txt')\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        total_score = 0\n",
    "        item_results = []\n",
    "        print(f\"\\n{'='*50}\\nIteration {iteration + 1}\\n{'='*50}\")\n",
    "        \n",
    "        print(\"Getting model answers and evaluations...\")\n",
    "        for item in dataset:\n",
    "            model_answer = await get_model_answer(item['instruction'], current_prompt)\n",
    "            evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "            score = extract_score(evaluation)\n",
    "            total_score += score\n",
    "            item_results.append({\n",
    "                'instruction': item['instruction'],\n",
    "                'model_answer': model_answer,\n",
    "                'expected_output': item['output'],\n",
    "                'evaluation': evaluation,\n",
    "                'score': score\n",
    "            })\n",
    "            print(f\"\\nInstruction: {item['instruction']}\")\n",
    "            print(f\"Model answer: {model_answer}\")\n",
    "            print(f\"Expected output: {item['output']}\")\n",
    "            print(f\"Evaluation: {evaluation}\")\n",
    "\n",
    "        avg_score = total_score / len(dataset)\n",
    "        print(f\"\\nAverage score for iteration {iteration + 1}: {avg_score:.2f}\")\n",
    "        \n",
    "        if iteration == 0:\n",
    "            initial_score = avg_score\n",
    "            \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_prompt = current_prompt\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= 3:\n",
    "            print(\"\\nNo improvement for 3 consecutive iterations. Stopping optimization.\")\n",
    "            break\n",
    "        if best_score >= 95:\n",
    "            print(\"\\High score reached. Stopping optimization.\")\n",
    "            break\n",
    "\n",
    "        last_scores.append(avg_score)\n",
    "\n",
    "        print(f\"Current best score: {best_score:.2f}\")\n",
    "        print(\"\\nGenerating improved prompt...\")\n",
    "        \n",
    "        worst_items = sorted(item_results, key=lambda x: x['score'])[:5]\n",
    "        \n",
    "        with open('./prompts/improvement_prompt_template.txt', 'r') as f:\n",
    "            improvement_prompt_template = f.read()\n",
    "        \n",
    "        improvement_prompt = improvement_prompt_template.format(\n",
    "            current_prompt=current_prompt,\n",
    "            avg_score=avg_score,\n",
    "            last_scores=[f\"{score:.2f}\" for score in last_scores],\n",
    "            worst_items=json.dumps(worst_items, indent=2)\n",
    "        )   \n",
    "        \n",
    "        print(f\"\\nFULL PROMPT:\\n{improvement_prompt}\")\n",
    "        \n",
    "        improved_prompt_response = await client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": improvement_prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        new_prompt = improved_prompt_response.choices[0].message.content\n",
    "        \n",
    "        # Remove content inside <analysis> tags\n",
    "        new_prompt_cleaned = remove_analysis(new_prompt)\n",
    "        \n",
    "        print(\"\\nModel's reasoning for the improved prompt:\")\n",
    "        print(new_prompt)\n",
    "        print(\"\\nCleaned prompt (with <analysis> content removed):\")\n",
    "        print(new_prompt_cleaned)\n",
    "        \n",
    "        # Save the new prompt for the next iteration\n",
    "        save_prompt(new_prompt_cleaned, f'prompt_iteration_{iteration + 1}.txt')\n",
    "        print(f\"Improved prompt saved to prompt_iteration_{iteration + 1}.txt\")\n",
    "        \n",
    "        # Use the cleaned prompt in the next iteration\n",
    "        current_prompt = new_prompt_cleaned\n",
    "\n",
    "    return best_prompt, best_score, initial_score\n",
    "\n",
    "# Modified main function\n",
    "async def main(max_iterations=2):\n",
    "    # Load dataset\n",
    "    with open('input/huy_dataset/huy_test.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Load initial prompt from file or use default\n",
    "    try:\n",
    "        initial_prompt = load_prompt('initial_prompt.txt')\n",
    "        print(f\"Loaded initial prompt: {initial_prompt}\")\n",
    "    except FileNotFoundError:\n",
    "        initial_prompt = \"You are a helpful AI assistant. Provide accurate and concise answers.\"\n",
    "        save_prompt(initial_prompt, 'initial_prompt.txt')\n",
    "\n",
    "    best_prompt, best_score, initial_score = await optimize_prompt(initial_prompt, dataset, max_iterations=max_iterations)\n",
    "\n",
    "    print(f\"\\n{'='*50}\\nOptimization Results\\n{'='*50}\")\n",
    "    print(f\"Initial score: {initial_score:.2f}\")\n",
    "    print(f\"Final score: {best_score:.2f}\")\n",
    "    print(f\"Improvement: {best_score - initial_score:.2f} points\")\n",
    "    print(f\"Percentage improvement: {((best_score - initial_score) / initial_score) * 100:.2f}%\")\n",
    "\n",
    "    # Save the best prompt\n",
    "    save_prompt(best_prompt, 'best_prompt.txt')\n",
    "    print(\"Best prompt saved to best_prompt.txt\")\n",
    "\n",
    "# Function to run the main coroutine\n",
    "def run_main(max_iterations=2):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return loop.run_until_complete(main(max_iterations))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_main()\n",
    "else:\n",
    "    # This allows the script to be run in both regular Python and Jupyter environments\n",
    "    print(\"To run the optimization, use: await main(max_iterations=2)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
