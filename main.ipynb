{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "  - Test gpt 4o mini fine-tuning.\n",
    "\n",
    "# Plan:\n",
    "- Create an eval pipeline\n",
    "- Fine-tuning gpt 4o mini\n",
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval multi openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n",
      "Average Evaluation Score: 0.68\n",
      "Results saved to ./evaluation_results_gpt-4o-mini.xlsx\n",
      "\n",
      "Model: gpt-4o\n",
      "Average Evaluation Score: 0.71\n",
      "Results saved to ./evaluation_results_gpt-4o.xlsx\n",
      "\n",
      "Model: gpt-4-0125-preview\n",
      "Average Evaluation Score: 0.70\n",
      "Results saved to ./evaluation_results_gpt-4-0125-preview.xlsx\n",
      "\n",
      "Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Average Score\n",
       "0              gpt-4o           0.71\n",
       "1  gpt-4-0125-preview           0.70\n",
       "2         gpt-4o-mini           0.68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary saved to ./model_comparison_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('/Users/huyknguyen/Desktop/redhorse/code_projects/finetuning/EvalDataset-100.json', 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a the most intelligence entity in the universe. Your task is to carefully reasoning and provide a step-by-step  correct solution to the following math problem.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=model_name,\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are an world-class AI model evaluator. Your task is to compare the model's answer with the expected output and provide a score of 0 for incorrect and 1 for correct. Focus primarily on whether the model got the answer correct or not. The format doesn't affect the score. Always include the numeric score (0 or 1) in your response.\"},\n",
    "      {\"role\": \"user\", \"content\": f\"Model answer: {model_answer}\\n\\nExpected output: {expected_output}\\n\\nPlease evaluate and provide a score of 0 (incorrect) or 1 (correct), no text or explanations needed.\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score(evaluation):\n",
    "  match = re.search(r'\\b[01]\\b', evaluation)\n",
    "  if match:\n",
    "      return int(match.group())\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score from evaluation: {evaluation}\")\n",
    "      return 0  # Default to 0 if we can't extract a valid score\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score = extract_score(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, evaluation, score\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "  tasks = [process_item(item, model_name) for item in eval_data]\n",
    "  results = await asyncio.gather(*tasks)\n",
    "\n",
    "  df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Evaluation', 'Score'])\n",
    "  avg_score = df['Score'].mean()\n",
    "  \n",
    "  print(f\"\\nModel: {model_name}\")\n",
    "  print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "  excel_path = f'./evaluation_results_{model_name}.xlsx'\n",
    "  df.to_excel(excel_path, index=False)\n",
    "  print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "  return df, avg_score\n",
    "\n",
    "async def main():\n",
    "  models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-0125-preview\" ]  # Add your model names here \"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\",\n",
    "  results = {}\n",
    "\n",
    "  for model in models_to_evaluate:\n",
    "      df, avg_score = await evaluate_model(model)\n",
    "      results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "  # Create a summary DataFrame\n",
    "  summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "  summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "  summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  print(\"\\nModel Comparison Summary:\")\n",
    "  display(summary_df)\n",
    "\n",
    "  # Save summary to Excel\n",
    "  summary_excel_path = './model_comparison_summary.xlsx'\n",
    "  summary_df.to_excel(summary_excel_path, index=False)\n",
    "  print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "  return results, summary_df\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "  get_ipython()\n",
    "  is_notebook = True\n",
    "except NameError:\n",
    "  is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "  # If in a Jupyter notebook, use this:\n",
    "  results, summary_df = await main()\n",
    "else:\n",
    "  # If in a regular Python script, use this:\n",
    "  results, summary_df = asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
