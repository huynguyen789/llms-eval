{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas openpyxl openai anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "  - Test gpt 4o mini fine-tuning.\n",
    "\n",
    "# Plan:\n",
    "- Create an eval pipeline\n",
    "- Fine-tuning gpt 4o mini\n",
    "- Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval multi openai models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step to make sure you get the correct answer.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=model_name,\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. \"},\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"\n",
    "       Model answer: {model_answer}\\n\\n\n",
    "       Expected output: {expected_output}\\n\\n\n",
    "       Your task is to compare the model's answer WITH THE EXPECTED OUTPUT and provide a super concise reason in one short sentence for the score, and then a score from 0 to 100. \n",
    "       Example: Reason: [super concise reason here]. Score: [score here]. \n",
    "       Use the following scale: 0 is completely wrong, 50 is missing half of the solution, 100 is completely correct, 80-90 if correct but missing some detail or not a complete answer. \n",
    "       Don't grade on formatting, as long as the answer is correct compare to the expected output. \n",
    "       If the logic is correct but the final answer is wrong, it's still wrong.\n",
    "       If the answer is correct but it has extra information, it's still correct. As long as the extra info is not completely wrong or hallucinated.\n",
    "       Do not grade by your knowledge, but grade based on the expected output. \n",
    "       Always include the numeric score (0-10) in your response.\n",
    "       \"\"\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score_and_reason(evaluation):\n",
    "  match = re.search(r'Reason:\\s*(.*?)\\s*Score:\\s*(\\d+|10)', evaluation, re.IGNORECASE | re.DOTALL)\n",
    "  if match:\n",
    "      reason = match.group(1).strip()\n",
    "      score = int(match.group(2))\n",
    "      return score, reason\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score and reason from evaluation: {evaluation}\")\n",
    "      return 0, \"Unable to extract reason\"  # Default values if extraction fails\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score, reason = extract_score_and_reason(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, score, reason\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "    tasks = [process_item(item, model_name) for item in eval_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Score', 'Reason'])\n",
    "    avg_score = df['Score'].mean()\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "    excel_path = f'{output_folder}/{dataset_name}_{current_time}_{model_name}.xlsx'\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "    return df, avg_score\n",
    "\n",
    "async def main():\n",
    "    models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"gpt-4o-2024-05-13\", \"gpt-4-0125-preview\"]  # Add your model names here\n",
    "    results = {}\n",
    "\n",
    "    for model in models_to_evaluate:\n",
    "        df, avg_score = await evaluate_model(model)\n",
    "        results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "    # Create a summary DataFrame\n",
    "    summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "    summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "    summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    # Save summary to Excel in the output folder\n",
    "    summary_excel_path = f'{output_folder}/model_comparison_summary.xlsx'\n",
    "    summary_df.to_excel(summary_excel_path, index=False)\n",
    "    print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "    return results, summary_df\n",
    "\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "# './input/huy_dataset/huy_test.json'\n",
    "# 'input/math-EvalDataset-10.json'\n",
    "#'./input/huy_dataset/huy_test2.json'\n",
    "\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "# Extract dataset name from input file path\n",
    "dataset_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "# Create output folder name\n",
    "current_time = datetime.now().strftime(\"%m%d%y_%I%M%p\")\n",
    "output_folder = f'./output/{dataset_name}_{current_time}'\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "    get_ipython()\n",
    "    is_notebook = True\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "    # If in a Jupyter notebook, use this:\n",
    "    results, summary_df = await main()\n",
    "else:\n",
    "    # If in a regular Python script, use this:\n",
    "    results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: gpt-4o-mini\n",
      "Average Evaluation Score: 78.12\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4o-mini.xlsx\n",
      "\n",
      "Model: gpt-4o\n",
      "Average Evaluation Score: 98.44\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4o.xlsx\n",
      "\n",
      "Model: gpt-4o-2024-08-06\n",
      "Average Evaluation Score: 94.06\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4o-2024-08-06.xlsx\n",
      "\n",
      "Model: gpt-4-0125-preview\n",
      "Average Evaluation Score: 86.25\n",
      "Results saved to ./output/huy_test_081024_1208PM/huy_test_081024_1208PM_gpt-4-0125-preview.xlsx\n",
      "\n",
      "Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>98.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>94.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>86.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>78.1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Average Score\n",
       "0              gpt-4o        98.4375\n",
       "1   gpt-4o-2024-08-06        94.0625\n",
       "2  gpt-4-0125-preview        86.2500\n",
       "3         gpt-4o-mini        78.1250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary saved to ./output/huy_test_081024_1208PM/model_comparison_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Step(BaseModel):\n",
    "  explanation: str\n",
    "  output: str\n",
    "\n",
    "class MathResponse(BaseModel):\n",
    "  steps: List[Step]\n",
    "  final_answer: str\n",
    "\n",
    "async def get_model_answer(instruction, model_name):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step and consider multi angles to make sure you get the correct and complete answer.\"},\n",
    "      {\"role\": \"user\", \"content\": instruction}\n",
    "  ]\n",
    "  \n",
    "  if model_name == \"gpt-4o-2024-08-06\":\n",
    "      response = await client.beta.chat.completions.parse(\n",
    "          model=model_name,\n",
    "          messages=messages,\n",
    "          response_format=MathResponse,\n",
    "      )\n",
    "      message = response.choices[0].message\n",
    "      if message.parsed:\n",
    "          steps_text = \"\\n\".join([f\"Step {i+1}: {step.explanation} Output: {step.output}\" for i, step in enumerate(message.parsed.steps)])\n",
    "          final_answer_text = f\"Final Answer: {message.parsed.final_answer}\"\n",
    "          combined_answer = f\"{steps_text}\\n{final_answer_text}\"\n",
    "          return combined_answer\n",
    "      else:\n",
    "          return message.refusal\n",
    "  else:\n",
    "      response = await client.chat.completions.create(\n",
    "          model=model_name,\n",
    "          messages=messages,\n",
    "          temperature=0.0,\n",
    "      )\n",
    "      return response.choices[0].message.content\n",
    "\n",
    "async def evaluate_answer(model_answer, expected_output):\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a world-class AI model evaluator. \"},\n",
    "      {\"role\": \"user\", \"content\": f\"\"\"\n",
    "      Model answer: {model_answer}\\n\\n\n",
    "      Expected output: {expected_output}\\n\\n\n",
    "      Your task is to compare the model's answer WITH THE EXPECTED OUTPUT and provide a super concise reason in one short sentence for the score, and then a score from 0 to 100. \n",
    "      Example: Reason: [super concise reason here]. Score: [score here]. \n",
    "      Use the following scale: 0 is completely wrong, 100 is completely correct, 80-90 if correct but missing detail or not a complete answer. \n",
    "      Don't grade on formatting, as long as the answer is correct compare to the expected output. \n",
    "      If the logic is correct but the final answer is wrong, it's still wrong.\n",
    "      If the answer is correct but it has extra information, it's still correct. As long as the extra info is not completely wrong or hallucinated.\n",
    "      Do not grade by your knowledge, but grade based on the expected output. \n",
    "      Always include the numeric score (0-100) in your response.\n",
    "      \"\"\"}\n",
    "  ]\n",
    "  response = await client.chat.completions.create(\n",
    "      model=\"gpt-4-0125-preview\",\n",
    "      messages=messages,\n",
    "      temperature=0.0,\n",
    "      tool_choice=None\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def extract_score_and_reason(evaluation):\n",
    "  match = re.search(r'Reason:\\s*(.*?)\\s*Score:\\s*(\\d+|100)', evaluation, re.IGNORECASE | re.DOTALL)\n",
    "  if match:\n",
    "      reason = match.group(1).strip()\n",
    "      score = int(match.group(2))\n",
    "      return score, reason\n",
    "  else:\n",
    "      print(f\"Warning: Could not extract score and reason from evaluation: {evaluation}\")\n",
    "      return 0, \"Unable to extract reason\"  # Default values if extraction fails\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "  model_answer = await get_model_answer(item['instruction'], model_name)\n",
    "  evaluation = await evaluate_answer(model_answer, item['output'])\n",
    "  score, reason = extract_score_and_reason(evaluation)\n",
    "  return item['instruction'], item['output'], model_answer, score, reason\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "  tasks = [process_item(item, model_name) for item in eval_data]\n",
    "  results = await asyncio.gather(*tasks)\n",
    "\n",
    "  df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Score', 'Reason'])\n",
    "  avg_score = df['Score'].mean()\n",
    "  \n",
    "  print(f\"\\nModel: {model_name}\")\n",
    "  print(f\"Average Evaluation Score: {avg_score:.2f}\")\n",
    "\n",
    "  excel_path = f'{output_folder}/{dataset_name}_{current_time}_{model_name}.xlsx'\n",
    "  df.to_excel(excel_path, index=False)\n",
    "  print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "  return df, avg_score\n",
    "\n",
    "async def main():\n",
    "  models_to_evaluate = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4o-2024-08-06\", \"gpt-4-0125-preview\"]  # Add your model names here\n",
    "  results = {}\n",
    "\n",
    "  for model in models_to_evaluate:\n",
    "      df, avg_score = await evaluate_model(model)\n",
    "      results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "  # Create a summary DataFrame\n",
    "  summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "  summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "  summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  print(\"\\nModel Comparison Summary:\")\n",
    "  display(summary_df)\n",
    "\n",
    "  # Save summary to Excel in the output folder\n",
    "  summary_excel_path = f'{output_folder}/model_comparison_summary.xlsx'\n",
    "  summary_df.to_excel(summary_excel_path, index=False)\n",
    "  print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "  return results, summary_df\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "  eval_data = json.load(f)\n",
    "\n",
    "# Extract dataset name from input file path\n",
    "dataset_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "# Create output folder name\n",
    "current_time = datetime.now().strftime(\"%m%d%y_%I%M%p\")\n",
    "output_folder = f'./output/{dataset_name}_{current_time}'\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "  get_ipython()\n",
    "  is_notebook = True\n",
    "except NameError:\n",
    "  is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "  # If in a Jupyter notebook, use this:\n",
    "  results, summary_df = await main()\n",
    "else:\n",
    "  # If in a regular Python script, use this:\n",
    "  results, summary_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval different models:\n",
    "- Eval pipeline for other models: google, anthropic, open sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class Step(BaseModel):\n",
    "  explanation: str\n",
    "  output: str\n",
    "\n",
    "class MathResponse(BaseModel):\n",
    "  steps: List[Step]\n",
    "  final_answer: str\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.beta.chat.completions.parse(\n",
    "  model=\"gpt-4o-2024-08-06\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n",
    "      {\"role\": \"user\", \"content\": \"* There is a three-digit number. The second digit is four times as big as the third digit, while the first digit is three less than the second digit. What is the number?\"},\n",
    "  ],\n",
    "  response_format=MathResponse,\n",
    ")\n",
    "\n",
    "message = completion.choices[0].message\n",
    "if message.parsed:\n",
    "  steps_markdown = \"\\n\".join([f\"**Step {i+1}:**\\n\\n{step.explanation}\\n\\n*Output:* {step.output}\\n\" for i, step in enumerate(message.parsed.steps)])\n",
    "  final_answer_markdown = f\"**Final Answer:** {message.parsed.final_answer}\"\n",
    "  \n",
    "  display(Markdown(steps_markdown))\n",
    "  display(Markdown(final_answer_markdown))\n",
    "else:\n",
    "  display(message.refusal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
