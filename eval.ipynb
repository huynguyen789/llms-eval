{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: gpt-4o-mini\n",
      "\n",
      "Model: gpt-4o-mini\n",
      "Average Score: 66.67\n",
      "Results saved to ./output/huy_test_031425_0145PM/huy_test_031425_0145PM_gpt-4o-mini.xlsx\n",
      "\n",
      "Evaluating model: gpt-4o\n",
      "\n",
      "Model: gpt-4o\n",
      "Average Score: 100.00\n",
      "Results saved to ./output/huy_test_031425_0145PM/huy_test_031425_0145PM_gpt-4o.xlsx\n",
      "\n",
      "Model Comparison Summary:\n",
      "         Model  Average Score\n",
      "0       gpt-4o     100.000000\n",
      "1  gpt-4o-mini      66.666667\n",
      "\n",
      "Summary saved to ./output/huy_test_031425_0145PM/model_comparison_summary_031425_0145PM.xlsx\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from anthropic import AsyncAnthropic\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "async_client = AsyncOpenAI()\n",
    "sync_client = OpenAI()\n",
    "anthropic_client = AsyncAnthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "nvidia_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ.get(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "async def get_openai_answer(instruction, model_name):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step and consider multiple angles to make sure you get the correct answer(s).\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Answer the following question. Provide your reasoning in <reasoning></reasoning> tags, and your final answer (A, B, C, or D) in <final_answer></final_answer> tags, only 1 letter answer, nothing else!.\\n\\n{instruction}\"}\n",
    "    ]\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        tool_choice=None\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_o1_answer(instruction, model_name):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"You are the most intelligent entity in the universe. Reasoning step by step and consider multiple angles to make sure you get the correct answer(s).\\n\\n Answer the following question. Provide your reasoning in <reasoning></reasoning> tags, and your final answer (A, B, C, or D) in <final_answer></final_answer> tags.\\n\\n{instruction}\"}\n",
    "    ]\n",
    "    response = sync_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_nvidia_answer(instruction, model_name):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are the most intelligent entity in the universe. Reasoning step by step and consider multiple angles to make sure you get the correct answer(s).\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Answer the following question. Provide your reasoning in <reasoning></reasoning> tags, and your final answer (A, B, C, or D) in <final_answer></final_answer> tags.\\n\\n{instruction}\"}\n",
    "    ]\n",
    "    response = nvidia_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def get_anthropic_answer(instruction, model_name):\n",
    "    message = await anthropic_client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"You are the most intelligent entity in the universe. Answer the following question. Provide your reasoning in <reasoning></reasoning> tags, and your final answer (A, B, C, or D) in <final_answer></final_answer> tags.\\n\\n{instruction}\",\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "def extract_final_answer(model_answer):\n",
    "    match = re.search(r'<final_answer>(.*?)</final_answer>', model_answer, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract final answer from model response: {model_answer}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "async def process_item(item, model_name):\n",
    "    if model_name.startswith(\"claude\"):\n",
    "        if not hasattr(process_item, \"anthropic_printed\"):\n",
    "            print(f\"Starting to get answers from Anthropic model: {model_name}\")\n",
    "            process_item.anthropic_printed = True\n",
    "        model_answer = await get_anthropic_answer(item['instruction'], model_name)\n",
    "    elif model_name.startswith(\"gpt\"):\n",
    "        if not hasattr(process_item, \"openai_printed\"):\n",
    "            print(f\"Starting to get answers from OpenAI model: {model_name}\")\n",
    "            process_item.openai_printed = True\n",
    "        model_answer = await get_openai_answer(item['instruction'], model_name)\n",
    "    elif model_name.startswith(\"o1\"):\n",
    "        if not hasattr(process_item, \"o1_printed\"):\n",
    "            print(f\"Starting to get answers from O1 model: {model_name}\")\n",
    "            process_item.o1_printed = True\n",
    "        model_answer = get_o1_answer(item['instruction'], model_name)\n",
    "    elif model_name.startswith(\"nvidia\"):\n",
    "        if not hasattr(process_item, \"nvidia_printed\"):\n",
    "            print(f\"Starting to get answers from NVIDIA model: {model_name}\")\n",
    "            process_item.nvidia_printed = True\n",
    "        model_answer = get_nvidia_answer(item['instruction'], model_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "    final_answer = extract_final_answer(model_answer)\n",
    "    score = 100 if final_answer == item['output'] else 0\n",
    "    return item['instruction'], item['output'], model_answer, final_answer, score\n",
    "\n",
    "async def evaluate_model(model_name):\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    # Reset the printed flags for each new model evaluation\n",
    "    process_item.anthropic_printed = False\n",
    "    process_item.openai_printed = False\n",
    "    process_item.o1_printed = False\n",
    "    process_item.nvidia_printed = False\n",
    "    tasks = [process_item(item, model_name) for item in eval_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Instruction', 'Expected Output', 'Model Answer', 'Final Answer', 'Score'])\n",
    "    avg_score = df['Score'].mean()\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Average Score: {avg_score:.2f}\")\n",
    "\n",
    "    excel_path = f'{output_folder}/{dataset_name}_{current_time}_{model_name}.xlsx'\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Results saved to {excel_path}\")\n",
    "\n",
    "    return df, avg_score\n",
    "\n",
    "async def main():\n",
    "    models_to_evaluate = [\n",
    "        # \"nvidia/llama-3.1-nemotron-70b-instruct\",\n",
    "        # \"o1-preview\",\n",
    "        # \"gpt-4-0125-preview\",\n",
    "        # \"claude-3-5-sonnet-20240620\",\n",
    "        \"gpt-4o-mini\", \n",
    "        \"gpt-4o\"\n",
    "        \n",
    "    ]\n",
    "    results = {}\n",
    "\n",
    "    for model in models_to_evaluate:\n",
    "        df, avg_score = await evaluate_model(model)\n",
    "        results[model] = {\"df\": df, \"avg_score\": avg_score}\n",
    "\n",
    "    # Create a summary DataFrame\n",
    "    summary_data = [(model, data[\"avg_score\"]) for model, data in results.items()]\n",
    "    summary_df = pd.DataFrame(summary_data, columns=[\"Model\", \"Average Score\"])\n",
    "    summary_df = summary_df.sort_values(\"Average Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    print(summary_df)\n",
    "\n",
    "    # Save summary to Excel in the output folder\n",
    "    summary_excel_path = f'{output_folder}/model_comparison_summary_{current_time}.xlsx'\n",
    "    summary_df.to_excel(summary_excel_path, index=False)\n",
    "    print(f\"\\nSummary saved to {summary_excel_path}\")\n",
    "\n",
    "    return results, summary_df\n",
    "\n",
    "# Load the evaluation dataset\n",
    "input_file_path = './input/huy_dataset/huy_test.json'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Extract dataset name from input file path\n",
    "dataset_name = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "\n",
    "# Create output folder name\n",
    "current_time = datetime.now().strftime(\"%m%d%y_%I%M%p\")\n",
    "output_folder = f'./output/{dataset_name}_{current_time}'\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Check if we're in a Jupyter notebook\n",
    "try:\n",
    "    get_ipython()\n",
    "    is_notebook = True\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "\n",
    "if is_notebook:\n",
    "    results, summary_df = await main()\n",
    "    \n",
    "else:\n",
    "    results, summary_df = asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
